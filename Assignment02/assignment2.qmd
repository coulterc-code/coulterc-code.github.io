
---
title: "Assignment 02 — Environment & PySpark Verification"
subtitle: "AD688 — EC2, Python, Quarto, and PySpark"
format:
  html: default
---

## Python & Package Versions

```{python}
import sys, platform
import pandas as pd, numpy as np
import sklearn, matplotlib
print("Python:", sys.version.split()[0])
print("OS:", platform.system(), platform.release())
print("pandas:", pd.__version__)
print("numpy:", np.__version__)
print("scikit-learn:", sklearn.__version__)
print("matplotlib:", matplotlib.__version__)
```

## PySpark Sanity Check

```{python}
import pyspark
from pyspark.sql import SparkSession
print("PySpark:", pyspark.__version__)

spark = SparkSession.builder.appName("assignment02-check").getOrCreate()

df = spark.createDataFrame(
    [(1, "alpha", 3.5),
     (2, "beta",  7.2),
     (3, "gamma", 1.1)],
    ["id", "label", "score"]
)

df.show()
print("Row count:", df.count())

from pyspark.sql import functions as F, Window
w = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
df2 = df.withColumn("score_z", (F.col("score") - F.avg("score").over(w)) / F.stddev(F.col("score")).over(w))
df2.show()

spark.stop()
```
 (3, "gamma", 1.1)],
    ["id", "label", "score"]
)

df.show()
print("Row count:", df.count())

from pyspark.sql import functions as F, Window
w = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
df2 = df.withColumn("score_z", (F.col("score") - F.avg("score").over(w)) / F.stddev(F.col("score")).over(w))
df2.show()

spark.stop()
```

